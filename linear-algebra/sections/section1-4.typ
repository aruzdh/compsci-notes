#import "../lib.typ": *

== Linear Dependence and Linear Independence

Suppose that $V$ is a vector space over an infinite field and that $W <= V$. Unless $W$ is the zero subspace, $W$ is an infinite set. It is desirable to find a "small" finite subset $S$ that generates $W$ because we can describe each vector in $W$ as a linear combination of the finite number of vectors in $S$. For example, consider a subspace $W$ that is generated by a set $S$, and let us attempt to find a proper subset of $S$ that also generates $W$. The search for this subset is related to the question of whether or not some vector in $S$ is a linear combination of the other vectors in $S$.

#definition(title: "Linearly Dependent Set")[
  A subset $S$ of a vector space $V$ is called *linearly dependent* if there exist a finite number of distinct vectors $u_1, u_2, dots, u_n in S$ and scalars $lambda_1, lambda_2, dots, lambda_n in KK$ (not all zero), such that 
  $ lambda_1 u_1 + lambda_2 u_2 + dots + lambda_n u_n = 0 $
  In this case we also say that the vectors of $S$ are linearly dependent.
]

#tip-box[
  Thus, rather than asking whether some vector in $S$ is a linear combination of the other vectors in $S$, it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in $S$ with coefficients that are *not all zero*.
]

  For any vectors $u_1, u_2, dots, u_n$, we have $lambda_1 u_1 + lambda_2 u_2 + dots + lambda_n u_n = 0$ if $lambda_1 = lambda_2 = dots = lambda_n = 0$. We call this the *trivial representation* of $0$ as a linear combination of the given vectors. Thus, for a set to be linearly dependent, there must exist a nontrivial representation of $0$ as a linear combination of the vectors in the set.

  #note-box[
    Consequently, any subset of a vector space that contains the zero vector is linearly dependent, because $ 0 = 1 dot 0 $ is a nontrivial representation of $0$ as a linear combination of vectors is the set.
  ]

  #example[
    In $M_(2 times 3)(RR)$, the set
    $ {
      mat(
        1,-3,2; -4,0,5
      )
      mat(
        -3,7,4;6,-2,-7
      ),
      mat(
        -2,3,11;-1,-3,2
      )
    } $
    is linear dependent because

    $
    5 mat(
      1,-3,2; -4,0,5
    ) +
    3 mat(
      -3,7,4;6,-2,-7
    ) +
    -2 mat(
      -2,3,11;-1,-3,2
    ) =
    mat(
      0,0,0;0,0,0
    )
    $
  ]

  #definition(title: "Linearly Independent")[
    A subset $S$ of a vector space that is not linearly dependent is called *linearly independent*. As before, we also say that the vectors of $S$ are linearly independent.
  ]

  #proposition[
    The following facts about linearly independent sets are true in any vector space.
    + The empty set is linearly independent, for linearly dependent sets must be nonempty.
    + A set consisting of a single nonzero vector is linearly independent. For if ${u}$ is linearly dependent, then $lambda u = 0$ for some $lambda != 0$. Thus $ u = a^(-1)(a u) = a^(-1)0 = 0$.
    + A set is linearly independent *if and only if* the *only* representation of the zero vector as linear combination of its vectors are trivial representations.
  ]

  #tip-box[
    The condition 3 provides a useful method for determining whether a finite set is linearly independent.
  ]

  #example[
    To prove that the set $ S = {(1,0,0,-1), (0,1,0,-1), (0,0,1,-1), (0,0,0,1)} $ is linearly independent, we must show that the only linear combination of vectors in $S$ that equals the zero vector is the one in which all the coefficients are zero. Suppose that $lambda_1, lambda_2, lambda_3$, and $lambda_4$ are scalars such that $ lambda_1(1,0,0,-1) + lambda_2(0,1,0,-1) + lambda_3(0,0,1,-1) + lambda_4 (0,0,0,1) = (0,0,0,0) $
    Equating the corresponding coordinates of the vectors on the left and the right sides of the equation, we obtain the following system of linear equations.
    $
    lambda_1 &= 0\
    lambda_2 &= 0\
    lambda_3 &= 0\
    -lambda_1 - lambda_2 - lambda_3 + lambda_4 &= 0\
    $
    Clearly, the only solution to this system is $lambda_1 = lambda_2 = lambda_3 = lambda_4 = 0$, and so $S$ is linearly independent 🤓☝🏽
  ]

  The following important results are immediate consequences of the definitions of linear dependence and linear independence.

  #theorem[
    Let $V$ be a vector space, and let $S_1 subset.eq S_2 subset.eq V$. If $S_1$ is linear dependent, then $S_2$ is linearly dependent.
  ]

  #corollary[
    Let $V$ be a vector space, and let $S_1 subset.eq S_2 subset.eq V$. If $S_2$ is linear independent, then $S_1$ is linearly independent.
  ]

  Earlier in this section, we remarked that the issue of whether $S$ is the smallest generating set for its span is related to the question of whether some vector in $S$ is a linear combination of the other vectors in $S$. Thus the issue of whether $S$ is the smallest generating set for its span is related to the question of whether $S$ is linearly dependent.

  Suppose that $S$ is any linearly dependent set containing two or more vectors. Then some vector $v in S$ can be written as a linear combination of the other vectors in $S$, and the subset obtain by removing $v$ from $S$ has the same span as $S$. It follows that if no proper subset of $S$ generates the span of $S$, the $S$ must be linearly independent. This idea is presented is the following theorem.

  #theorem[
    Let $S$ be a linearly independent subset of a vector space $V$, and let $v in V$ and $v in.not S$. Then $S union {v}$ is linearly dependent if and only if $v in "span"(S)$.
  ]

