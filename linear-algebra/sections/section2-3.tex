\section{Composition of linear transformations and matrix multiplication}
In the previous section we learned how to associate a matrix with a linear transformation in such a way that both sums and scalar multiples of matrices are associated with the corresponding sums and sums and scalar multiples of the transformations. The question now arises as to how the matrix representation of a composite of linear transformations is related to the matrix representation of each of the associated linear transformations. The attempt to answer this question leads to a definition of matrix multiplication. We use the more convenient notation of $UT$ rather than $U \circ T$ for the composite or linear transformations \textit{U} and \textit{T}.

Our first result shows that the composite of linear transformations is linear.

\thm{}{
  Let \textit{V, W, Z} be vector spaces over the same field \textit{K}, and let $T : V \longrightarrow W$ and $U : W \longrightarrow Z$ be linear. Then $UT : V \longrightarrow Z$, that is $(UT)(v) = U(T(v)),$ is linear.
}

The following theorem lists some of the properties of the composition of linear transformations.

\thm{}{
  Let \textit{V} be a vector space. Let $T, U_1, U_2 \in \mathcal L(V)$. Then 
  \begin{itemize}
    \item $T(U_1 + U_2) = TU_1 + TU_2$ and $(U_1 + U_2)T = U_1T + U_2T$
    \item $T(U_1U_2) = (TU_1)U_2$
    \item $TI = IT = T$ where $I$ is the identity transformation.
    \item $\lambda (U_1U_2) = (\lambda U_1) U_2 = U_1 (\lambda U_2),~ \forall \lambda \in K$
  \end{itemize}
}

\nt{
  A more general result holds for linear transformations that have domains unequal to their codomains.
}

Let $T : V \longrightarrow W$ and $U : W \longrightarrow Z$ be linear transformations, and let $A = [U]_\beta^\gamma$ and $B = [T]_\alpha^\beta$ where $\alpha = \{v_1, v_2, \cdots, v_n\},~ \beta = \{w_1, w_2, \cdots, w_n\}$, and $\gamma = \{z_1, z_2, \cdots, z_n\}$ are ordered bases for \textit{V, W}, and \textit{Z}, respectively. We would like to define the product $AB$ of two matrices so that $AB = [UT]_\alpha^\gamma$. Consider the matrix $[UT]_\alpha^\gamma$. For $1 \le j \le n$, we have
\begin{align*}
  (UT)(v_j) &= U(T(v_j)) = U(\sum_{k = 1}^m B_{kj} w_k) = \sum_{k = 1}^m B_{kj} U(w_k)\\
            &= \sum_{k = 1}^m B_{kj} (\sum_{i = 1}^p A_{ik} z_i) = \sum_{i = 1}^p(\sum_{k = 1}^m A_{ik} B_{kj})z_i\\
            &= \sum_{i = 1}^p C_{ij} z_i,
\end{align*}
where
\[C_{ij} = \sum_{k = 1}^m A_{ik} B_{kj}\]

This computation motivates the following definition of matrix multiplication.

\dfn{}{
  Let \textit{A} be an $m \times n$ matrix and \textit{B} be an $n \times p$ matrix. We define the \textbf{product} of \textit{A} and \textit{B}, denoted $AB$, to be the $m \times p$ matrix such that  
  \[(AB)_{ij} = \sum_{k = 1}^n A_{ik} B_{kj},~ \text{for } 1 \le i \le m, \qquad 1 \le j \le p. \]
}

\nt{
  Note that $(AB)_{ij}$ is the sum of products of corresponding entries from the \textit{ith} row of \textit{A} and the \textit{jth} colummn of \textit{B}.
}

The following mnemonic device is helpful:
\["(m \times n) \cdot (n \times p) = (m \times p)";\]
that is, in order for the product $AB$ to be defined, the two "inner" dimensions muts be equal, and the two "outer" dimensions yield the size of the product.

\ex{}{
  We have
\[
  \begin{pmatrix}
    1 & 2 & 1\\
    0 & 4 & -1
  \end{pmatrix}
  \begin{pmatrix}
    4\\[0.5ex]
    2\\[0.5ex]
    5
  \end{pmatrix}
  =
  \begin{pmatrix}
    1\cdot4 + 2\cdot2 + 1\cdot5\\[0.75ex]
    0\cdot4 + 4\cdot2 + (-1)\cdot5
  \end{pmatrix}
  =
  \begin{pmatrix}13\\3\end{pmatrix}.
\]
  Notice again the symbolic relationship
  \[
    (2\times 3)\,\cdot\,(3\times 1) \;=\; 2\times 1.
  \]
}

\nt{
  We have that matrix multiplication is not commutative.
}

The next theorem is an immediate consequence of our definition of matrix multiplication.

\thm{}{
  Let \textit{V, W, Z} be finite-dimensional vector spaces with ordered bases $\alpha, \beta$, and $\gamma$, respectively. Let $T : V \longrightarrow W$ and $U : W \longrightarrow Z$ be linear transformations. Then
  \[[UT]_\alpha^\gamma = [U]_\beta^\gamma [T]_\alpha^\beta\]
}

\cor{}{
  Let \textit{V} be a finite-dimensional vector space with an ordered basis $\beta$. Let $T,U \in \mathcal L(V)$. Then
  \[[UT]_\beta = [U]_\beta [T]_\beta\]
}

\nt {
  In the prior scenario, we use $[T]_\beta^\beta = [T]_\beta$
}

\thm{}{
  Let \textit{A} be an $m \times n$ matrix, \textit{B} and \textit{C} be $n \times p$ matrices, and \textit{D} and \textit{E} be $q \times m$ matrices. Then

  \begin{enumerate}[a)]
    \item $A(B + C) = AB + AC$ and $(D + E)A = DA + EA$
    \item $\lambda (AB) + (\lambda A)B = A(\lambda B),~ \forall \lambda \in K$
    \item $I_m A = A = A I_n$
    \item If \textit{V} is an \textit{n}-dimensional vector space with an ordered basis $\beta$, then $[I_V]_\beta = I_n$
  \end{enumerate}
}

\thm{}{
  Let \textit{V} and \textit{W} be finite-dimensional vector spaces having ordered bases $\beta$ and $\gamma$, respectively, and let $T : V \longrightarrow W$ be linear. Then, for each $u \in V$, we have
  \[[T(u)]_\gamma = [T]_\beta^\gamma [u]_\beta\]
}

\ex{}{

}

We complete this section with the introduction of the \textit{left-multiplication transformation} $L_A$, where \textit{A} is an $m \times n$ matrix. This transformation is probably the most important tool for transferring properties about transformations to anologous properties about matrices and vice versa.

\dfn{}{
  Let \textit{A} be an $m \times n$ matrix with entries from a field \textit{K}. We denote by $L_A$ the mapping $L_A : K^n \longrightarrow K^m$ defined by $L_A(x) = Ax$ (the matrix preduct of \textit{A} and \textit{x}) for each colummn vector $x \in K^n$. We call $L_A$ a \textbf{left-multiplication transformation}.
}

\ex{}{

}

We see in the next theorem that not only is $L_A$ linear,but, in fact, it has a great many other useful properties. These properties are all quite natural and so are easy to remember.

\thm{}{
  Let \textit{A} be an $m \times n$ matrix with entries from \textit{K}. Then the left-multiplication transformation $L_A : K^n \longrightarrow K^m$ is linear. Furthermore, if \textit{B} is any other $m \times n$ matrix (with entries from \textit{K}) and $\beta$ and $\gamma$ are the standard ordered bases for $K^n$ and $K^m$, respectively, then we have the following properties.

  \begin{enumerate}[a)]
    \item $[L_A]_\beta^\gamma = A$
    \item $L_A = L_B \Longleftrightarrow A = B$
    \item $L_{A + B} = L_A + L_B$ and $L_{\lambda A} = \lambda L_A, ~\forall \lambda \in K$
    \item If $T : K^n \longrightarrow K^m$ is linear, then there exists a unique $m \times n$ matrix \textit{C} such that $T = L_C$. In fact, $C = [T]_\beta^\gamma$
    \item If \textit{E} is an $n \times p$ matrix, then $L_{AE} = L_A L_E$
    \item If $m = n$, then $L_{I_n} = I_{K^n}$
  \end{enumerate}
}

We now use left-multiplication transformations to establish the associativity of matrix multiplication.
\thm{}{
  Let \textit{A, B}, and \textit{C} be matrices such that $A(BC)$ is defined. Then $(AB)C$ is also defined and $A(BC) = (AB)C$; that is, matrix multiplication is associative.
}
