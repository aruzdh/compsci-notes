\section{Distribuciones Absolutamente Continuas}
Dada una variable aleatoria $\mcX$ \textit{continua}, su correspondiente función de distribución es \textbf{continua}. Se tienen los siguientes casos:
\begin{itemize}
  \item Singulares: La función de densidad \textbf{no existe}. 
  \item Absolutamente continuas: \[F_{\mcX}'(x) = f_{\mcX}(x)\] \[P(a < \mathcal X < b) = \int_a^b f_{\mcX}(x)\,dx\]
\end{itemize}

\subsection{Distribución Uniforme Continua}
\dfn{Distribución Uniforme Continua}{
  Una \textit{variable aleatoria $\mathcal X$} tiene \textbf{distribución Uniforme Continua} y se denota $\mathcal X \sim Unif(a,b)$ si su \textbf{función de densidad} es la siguiente.
  \[f_{\mcX}(x) = 
    \begin{cases}
      \frac{1}{b-a} & \text{si } x \in (a,b)\\
      0 & \text{ otro caso}
    \end{cases}
    = f_{\mcX}(x) = \frac{1}{b-a}\mathbb 1_{(a,b)}^{(x)}
    \]
}

\nt{
  Se observa que $f(x) \ge 0$ y $\int_{-\infty}^{\infty}f_{\mcX}(x) \,dx = 1$
  \begin{align*}
    \int_{-\infty}^{\infty} f_{\mcX}(x)\, \mathbb 1_{(a,b)}^{(x)} \,dx &= \int_{-\infty}^a \frac1{b-a} \cdot 0 \,dx + \int_a^b \frac1{b-a}\,dx + \int_b^{\infty}\frac1{b-a}\cdot 0 \,dx\\
    &= \int_{a}^b \frac{dx}{b-a} \\
    &= \frac1{b-a}\int_{a}^b dx = \frac1{b-a} (b-a) = \frac{b-a}{b-a} = 1
  \end{align*}
}

Calculemos la función de la distribución Uniforme.

\[F_{\mathcal X}(c) = P(\mathcal X \le c) = \int_{-\infty}^c \frac1{b-a} \mathbb 1_{(a,b)}^{(x)}~dx\]

Si $c \le a$
\[F_{\mathcal X}(c) = \int_{-\infty}^c \frac1{b-a} \mathbb 1_{(a,b)}^{(x)}~dx = \int_{-\infty}^c \frac1{b-a} \cdot 0~dx = 0\]

Si $c \in (a,b)$
\[F_{\mathcal X}(c) = \int_{-\infty}^c \frac1{b-a} \mathbb 1_{(a,b)}^{(x)}~dx = \int_{a}^c \frac1{b-a}~dx = \frac{c-a}{b-a} \]

Si $c \ge b$
\[F_{\mathcal X}(c) = \int_{-\infty}^b \frac1{b-a} \mathbb 1_{(a,b)}^{(x)}~dx + \int_b^c \frac1{b-a} \mathbb 1_{(a,b)}^{(x)}~dx = \int_a^b \frac1{b-a} ~dx = 1\]

\dfn{Función de Distribución}{
  Dada $\mathcal X \sim Unif(a,b)$. Su \textbf{función de distribución} está dada por
  \[
    F_{\mathcal X}(c) =
    \begin{cases}
      0 &\text{si } c \le a\\
      \frac{c-a}{b-a} &\text{si } c \in (a,b)\\
      1 &\text{si } c \ge b
    \end{cases}
  \]
}

\subsection{Distribución Normal Estándar o Gaussiana}

\dfn{Distribución Normal Estándar}{
  Una \textit{variable aleatoria $\mathcal X$} tiene \textbf{distribución Normal Estándar} si su función de densidad es \[\phi(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}},\, x \in \mathbb R\]
}

\nt{
  Se tiene que
  \[
    \phi(x) \ge 0 \qquad \text{y} \qquad \int_{-\infty}^{\infty}\phi(x)\,dx = 1
  \]
  Pero recordemos que la función de densidad Normal
  \[\phi(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}\]
  \textbf{no} tiene antiderivada.
}

Si $\mathcal X \sim N(0,1)$, entonces $\mathcal X$ es \textbf{variable aleatoria normal estándar}.

Si $\mathcal X \sim N(y,\sigma)$, entonces $\mathcal X$ es \textbf{variable aleatoria normal con parámetros} $y \in \mathbb R$ y $\sigma \neq 0$.

\nt{
  Observemos que si $\mathcal X \sim N(0,1)$ entonces $y \equiv 0,~ \sigma \equiv 0$
}

Cuando hablamos de esta distribución, usualmente se trabaja con una \textit{Tabla de la Distribución Normal}

\ex{}{
  Sea $\mathcal X \sim N(-1,3)$, donde $y = -1, ~ \sigma = 3$
  Entonces $\mathcal X = -1 + 3 \mathcal Z$, con $\mathcal Z\sim N(0,1)$

  Se tiene que
  \begin{align*}
    P(\mathcal X \le -1.06) &= P(-1 + 3 \mathcal Z \le -1.06)\\
                 &= P(3 \mathcal Z \le -1.06 + 1)\\
                 &= P(\mathcal Z \le \frac{-1.06+1}{3})\\
                 &= P(\mathcal Z \le 0.02)\\
                 &= 0.5080
  \end{align*}
  Los valores como $0.5080$ son obtenidos de la tabla mencionada.
}

\subsection{Distribución Exponencial}
Esta distribución es el \textit{equivalente continuo} a la distribución geométrica. Entonces, esta distribución modela el tiempo que transcurre hasta que ocurre un evento determinado.

\dfn{Distribución Exponencial}{
  Una variable aleatoria $\mathcal X$ tiene \textbf{distribución Exponencial} con parámetros $\lambda > 0$ si su función de densidad es
  \[f_{\mcX}(x) = 
    \begin{cases}
      \lambda e^{-\lambda x} & x>0\\
      0 & x \le 0
    \end{cases}
  \]
  Se donota como $\mathcal X \sim exp(\lambda)$
}

\subsection{Distribución Gamma}

Definamos una función $g$ de la siguiente manera.
\[g(x) = 
  \begin{cases}
    x^{\alpha - 1}e^{-\lambda x} & x > 0\\
    0 & \text{otro caso}
  \end{cases}
\]
Para que \textit{g} sea integrable, requerimos que $\alpha, \lambda > 0$.\\
Notemos que $g(x) \ge 0,~ \forall x \in \mathbb R$ (su soporte son los números positivos)\\
Ahora encontremos el valor de \textit{c} tal que
\[\int_{-\infty}^{\infty}cg(x)~dx = 1\] Se tiene que

\begin{align*}
  \int_{-\infty}^{\infty}cg(x)\,dx &= \int_{-\infty}^{\infty}c x^{\alpha -1}e^{-\lambda x} \, \mathbb 1_{(0, \infty)}^{(x)} \,dx = c\int_0^{\infty} x^{\alpha -1}e^{-\lambda x} \,dx\\
                                  &= c\int_0^{\infty}(\frac  u\lambda)^{\alpha-1}e^{-u}\, \frac{du}\lambda = \frac c{\lambda^\alpha} \int_0^{\infty} u^{\alpha - 1}e^{-u}\,du
\end{align*}
Dado que buscamos 
\[\frac c{\lambda^\alpha} \int_0^{\infty} u^{\alpha - 1}e^{-u}~du = 1\] y que 
\[\int_0^{\infty} u^{\alpha - 1}e^{-u}~du\] es la \textbf{Función Gamma} $\Gamma(\alpha)$
\[\boxed{\therefore \,c = \frac{\lambda^\alpha}{\Gamma(\alpha)}}\]

\dfn{Densidad Gamma}{
  La siguiente función se conoce como la \textbf{densidad gamma}.
  \[g(x) =
    \begin{cases}
      \frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x} &x >0\\
      0 & x \le 0\\
    \end{cases}
  \]
  donde
  \[\Gamma(\alpha) = \int_0^{\infty} u^{\alpha - 1}e^{-u}~du\]
}

\dfn{Distribución Gamma}{
  Dada una variable aleatoria que tenga la densidad gamma. $\mathcal X$ tiene \textbf{distribución Gamma} con parámetros $\lambda > 0$ y $\alpha > 0$ denotado por $\mathcal X \sim \Gamma(\alpha, \lambda)$
}

\nt{
  Si tomamos $\alpha = 1$ se tiene que
  \[\Gamma(1) = \int_0^{\infty} u^{1 - 1}e^{-u}~du = \int_0^{\infty} e^{-u}~du = - e^{-u}\Big|_0^{\infty} = 1\]
  Reemplazando en la densidad se obtiene
  \[g(x) =
    \begin{cases}
      \frac{\lambda^1}{\Gamma(1)}x^{1-1}e^{-\lambda x} &x >0\\
      0 & x \le 0\\
    \end{cases}
  \]
  Por tanto
  \[g(x) =
    \begin{cases}
      \lambda e^{-\lambda x} &x >0\\
      0 & x \le 0\\
    \end{cases}
  \]
  Donde la función obtenida es la \textit{distribución exponencial}. Así que la distribución exponencial es un caso particular de la densidad gamma.
}

Las siguientes son propiedades de la \textit{Función Gamma} (no necesariamente de la densidad gamma).

\begin{itemize}
  \item $\Gamma(1) = 1$
    \begin{myproof}
      Ejercicio.
    \end{myproof}
  \item $\Gamma(1, \lambda) \sim exp(\lambda)$
    \begin{myproof}
      Ejercicio.
    \end{myproof}
  \item $\Gamma(\frac12) = \sqrt \pi$
    \begin{myproof}
      Ejercicio.
    \end{myproof}
  \item $\Gamma(x + 1) = x \Gamma(x),~ \forall x\in \mathbb R^+$
    \begin{myproof}
      Ejercicio.
    \end{myproof}
  \item Si $n \in \mathbb N \Rightarrow \Gamma(n + 1) = n!$
    \begin{myproof}
      Ejercicio.
    \end{myproof}
\end{itemize}

\subsection{Variables Aleatorias Simétricas}

\dfn{Función de Densidad Simétrica}{
  Una función de densidad $f$ es \textbf{simétrica} si $f$ es una función \textit{par}, es decir, \[f(x) = f(-x),\, \forall x \in \mathbb R\]
}

\dfn{Variable Aleatoria Simétrica}{
  Una \textit{variable aleatoria $\mathcal X$} es \textbf{simétrica} si $\mathcal X$ y $-\mathcal X$ tienen la misma función de distribución, es decir,
  \[F_{\mathcal X}(a) = F_{-\mathcal X} (a),~ \forall a \in \mathbb R\]
  Lo anterior se denota como ($\mathcal X \stackrel{\text{d}}{\sim} \mathcal {-X}$)
}

\thm{}{
  Una \textit{variable aleatoria $\mathcal X$} \textbf{es simétrica si y solo si su función de densidad es simétrica} (su función de densidad es par).
}

\begin{myproof}
  \boxed{\Leftarrow} Supangamos que la función de densidad es par (simétrica). \\Buscamos probar que $F_{\mathcal {-X}}(a) = F_{\mathcal X}(x),~ \forall a \in \mathbb R$\\
  Observemos que $w \in \{-\mathcal X \le a\},~ -\mathcal X (w)\le a \Leftrightarrow \mathcal X(w) \ge -a$
  \[F_{-\mathcal X}(a) = P(-\mathcal X \le a) = P(\mathcal X \ge -a) = \int_{-a}^{\infty} f(u)~du\] donde $u = -v$ y $du = -dv$

  \[F_{-\mathcal X}(a) = - \int_{a}^{-\infty} = f(-v)~dv = \int_{-\infty}^a = f(-v)~dv = \int_{-\infty}^a = f(v)~dv = P(\mathcal X \le a) = F_{\mathcal X}(a)\]
  \begin{center}
    $\therefore \, \mathcal X$ es simétrica.
  \end{center}
  \boxed{\Rightarrow} Ejercicio.

\end{myproof}

\ex{Variable Aleatoria Simétrica} {
  Dada $\mathcal X \sim N(0,1)$. $\mcX$ es una \textit{variable aleatoria simétrica} porque $\phi(x) = \phi(-x)$
}

\mlemma{}{
  Si $\mathcal X$ es una \textit{variable aleatoria simétrica}, entonces \[F_{\mathcal X}(0) = \frac{1}{2}\]
}

\begin{myproof}
  \[F_{\mathcal X}(0) = P(\mathcal X \le 0) = 1 - P(\mathcal X \ge 0) = 1 - P(-\mathcal X \ge 0) = 1 - P(\mathcal X \le 0)\]
  Este hecho se justfica porque
  \[w \in \{-\mathcal X  \ge 0\} \Leftrightarrow -\mathcal X(w) \ge 0 \Leftrightarrow \mathcal X(w) \le 0 \Leftrightarrow w \in \{\mathcal X \le 0\}\]
  Entonces
  \[P(\mathcal X \le 0) = 1 - P(\mathcal X \le 0) \Rightarrow 2P(\mathcal X \le 0)= 1 \Rightarrow P(\mathcal X \le 0) = \frac12\]
\end{myproof}

\mlemma{}{
  Si $\mathcal X$ es una \textit{variable aleatoria simétrica}, entonces \[F_{\mathcal X}(-a) = 1 - F_{\mathcal X}(a)\]
}

\begin{myproof}
  \[F_{\mathcal X}(-a) = P(\mathcal X \le -a) = P(-\mathcal X \le -a) = P(\mathcal X \ge a)= 1 - P(\mathcal X \le a) = 1 - F_{\mathcal X}(a)\]
\end{myproof}

\subsection{Teorema de Cambio de Variable}
\thm{Teorema de Cambio de Variable}{
  Sea $\mathcal X$ una \textit{variable aleatoria absolutamente continua} con valores en $(a,b) \subseteq \mathbb R$. Sea $g : (a,b) \longrightarrow \mathbb R$ una función continua estrictamente creciente o decreciente con inversa diferenciable. Entonces la variable aleatoria $\mathcal Y= g(\mathcal X) = g \circ \mathcal X : \Omega \longrightarrow \mathcal R$ tiene la función de densidad siguiente.

  \[f_{\mathcal Y}(y) = 
    \begin{cases}
    f_{\mathcal X} (g^{-1}(y)) \abs*{\frac{d}{dy} g^{-1}(y)}, &y \in g(a,b)\\
      0, & \text{otro caso}
    \end{cases}
  \]
}

\begin{myproof}
  Tan evidende que no se muestra (es mucho, está en las notas).
\end{myproof}

\ex{Teorema de Cambio de Variable}{
  Sea $\mathcal X$ una \textit{variable aleatoria} Normal Estándar, y definimos $\mathcal Y = y + \sigma \mathcal X$ con $y \in \mathbb R,~ \sigma > 0$.
  \newpara
  Calculemos la función de densidad de $\mathcal Y$

  Se tiene $\mathcal Y = g(\mathcal X)$ donde $\mathcal X$ es Normal Estándar
  \[f_{\mathcal X}(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}\]

  Notemos que
  \[
    g(s) = y + \sigma s \Longrightarrow g^{-1}(s) = \frac{s-y}{\sigma}
  \]

  Aplicando el teorema de cambio de variable.
  \begin{align*}
    f_{\mathcal Y}(x) &= f_{\mathcal X}(g^{-1}(x)) \mid \frac{d}{dx} g^{-1}(x) \mid\\
             &= \frac{e^{-\frac{g^{-1}(x)^2}{2}}}{\sqrt{2\pi}}\mid \frac{d}{dx} g^{-1}(x) \mid\\
             &= \frac{e^{-\frac{1}{2} (\frac{x - y}{\sigma})^2}}{\sqrt{2\pi}}\mid \frac{d}{dx} (\frac{x-y}{\sigma}) \mid\\
             &= \frac{e^{-\frac{1}{2}(\frac{x-y}{\sigma})^2}}{\sqrt{2\pi}\sigma}
  \end{align*}

  Entonces, la función de densidad de $\mathcal Y = y + \sigma \mathcal X, ~ y \in \mathbb R,~ \sigma > 0$ es 
  \[f_{\mathcal Y}(x) = \frac{e^{-\frac{1}{2}(\frac{x-y}{\sigma})^2}}{\sqrt{2\pi}\sigma} \]

  Por tanto, $\mathcal Y$ es una variable aleatoria con distribución Normal
}

\subsection{Propiedad de Pérdida de Memoria}

La variable aleatoria exponencial y la geométrica son las unicas que satisfacen la siguiente propiedad.

Sea $\mathcal X \sim exp(\lambda)$ y sean $a,b \ge 0$. Entonces se satisface lo siguiente.
\[P(\mathcal X > a + b \mid \mathcal X > b) = P(\mathcal X > a)\]

Lo anterior se puede entender como "la probabilidad de que un evento ocurra en el futuro no depende del tiempo transcurrido sin que este evento haya sucedido". Así, este tipo de variables aleatorias nos permiten modelar fenomenos donde su riesgo de ocurrencia no cambia con el tiempo.

\nt{
  Dado que estamos trabajando con probabilidad condicional, se tiene lo siguiente.
  \[P(\mathcal X > a + b \mid \mathcal X > b) = \frac{P(\mathcal X > a + b ,\, \mathcal X > b)}{P(X > b)} = \frac{P(\mathcal X > a + b)}{P(X > b)} = P(\mathcal X > a)\]
  Por tanto
  \[P(\mathcal X > a + b) = P(\mathcal X > a)~P(\mathcal X > b)\]
  Lo anterior se justfica de manera gráfica y usando la propiedad de perdida de memoria.
}

\thm{}{
  Sea $\mathcal X$ una variable aleatoria tal que satisface
  \[P(\mathcal X > a + b \mid \mathcal X > b) = P(\mathcal X > a) \quad \text{con } a,b \ge 0\]
  Entonces
  \[P(\mathcal X > 0) = 0 \qquad \text{o} \qquad \mathcal X \sim exp(\lambda)\]
}


